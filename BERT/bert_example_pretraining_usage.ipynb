{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrates simple usage for pretraining Bert; \n",
    "* Using Masked Language Modelling and Next Sentence Prediction as pretraining tasks;\n",
    "* I also use the the top 2 books from \"yesterday\" from project Gutenberg;\n",
    "* Details of how I did the data processing can be found in the bert_data_processing.py file;\n",
    "* Details of implementaion of BertModel can be foun in the bert.py file;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f617003c5b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import bert\n",
    "import bert_data_processing as bdp\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file with that name already exists, if truncate is True I will overwrite it. Continue [y/n]:y\n",
      "downloading 2 books...\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = bdp.get_gutenberg_loader_and_vocab(batch_size, max_len, \n",
    "                                                       \"gutenberg_books.txt\", \n",
    "                                                       num_books=2,\n",
    "                                                       truncate=True, min_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a BertModel instance;\n",
    "* Uses Next Sentence Prediction as well as Masked Language Modelling as pretraining tasks;\n",
    "* Hence, there will be some CrossEntropy losses involved later in the notebook;\n",
    "* This initialisation is somewhat motivated by bert_small;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, hidden_dim, ffn_hidden, num_heads = len(vocab), 768, 1024, 4\n",
    "norm_dim, ffn_input, num_layers, dropout, with_bias = [768], 768, 2, 0.2, True\n",
    "\n",
    "bert_model = bert.BertModel(hidden_dim, hidden_dim, hidden_dim, hidden_dim, num_heads,\n",
    "                            norm_dim, ffn_input, ffn_hidden, num_layers, vocab_size, pos_encoding_size=1000,\n",
    "                            mlm_input=hidden_dim, mlm_hiddens=2*hidden_dim, nsp_input=hidden_dim,\n",
    "                            nsp_hidden=2*hidden_dim, dropout=dropout, with_bias=with_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get two losses for NSP and MLM;\n",
    "* Since the sequences in the dataset have been padded to be of the same length, I will use weights when calculating the loss from MLM.\n",
    "* A weight of *0* for <pad\\> tokens and *1* for real tokens;\n",
    "* The reduction for the MLM loss is *none*, therefore, and is averaged over the weights manually;\n",
    "* For the NSP task the reduction is the default one, i.e. *reduction=\"mean\"*;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mlm = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "loss_nsp = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I choose to optimise with Adam;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_optim = torch.optim.Adam(bert_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoding: torch.Size([512, 64, 768]) --> (batch_size, seq_len, embed_size)\n",
      "shape of mlm_preds: torch.Size([512, 10, 1816]) --> (batch_size, num_masks, vocab_size)\n",
      "shape of nsp_preds: torch.Size([512, 2]) --> (batch_size, num_classes_to_predict)\n"
     ]
    }
   ],
   "source": [
    "# check the general format of the output from the BertModel instance forward call;\n",
    "with torch.no_grad():\n",
    "    for (tokens, segments, attention_masks, masked_positions, weights_for_masks,\n",
    "         original_labels_for_masks, nsp_labels) in train_iter:\n",
    "        encodings, mlm_preds, nsp_preds = bert_model(tokens, segments, attention_masks, \n",
    "                                                     masked_positions)\n",
    "        break\n",
    "\n",
    "print(f\"shape of encoding: {encodings.shape} --> (batch_size, seq_len, embed_size)\")\n",
    "print(f\"shape of mlm_preds: {mlm_preds.shape} --> (batch_size, num_masks, vocab_size)\")\n",
    "print(f\"shape of nsp_preds: {nsp_preds.shape} --> (batch_size, num_classes_to_predict)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the losses from the pretraining tasks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_per_batch(bert_model, loss_mlm, loss_nsp, vocab_size, tokens, segments,\n",
    "                         attention_masks, masked_positions, weights_for_masks, \n",
    "                         original_labels_for_masks, nsp_labels):\n",
    "    \n",
    "    # effectively the forward pass;\n",
    "    encodings, mlm_preds, nsp_preds = bert_model(tokens, segments,\n",
    "                                          attention_masks, masked_positions)\n",
    "    # Now get loss from MLM task;\n",
    "    # it is important that the loss reduction is none, so that I can customize it a bit\n",
    "    # by multiplying by weights so that I don't count the loss from <pad> tokens\n",
    "    # when doing the MLM model;\n",
    "    mlm_loss = loss_mlm(mlm_preds.reshape(-1, vocab_size), \n",
    "                    original_labels_for_masks.reshape(-1)) * weights_for_masks.reshape(-1)\n",
    "    \n",
    "    # now I can average the MLM loss over the weights;\n",
    "    mlm_loss = mlm_loss.sum() / (weights_for_masks.sum() + 1e-9)\n",
    "    \n",
    "    # Now get the next sentence prediction loss;\n",
    "    # here this is the default reduction for loss_nsp, i.e. averaged;\n",
    "    nsp_loss = loss_nsp(nsp_preds, nsp_labels)\n",
    "    \n",
    "    # combine the two losses;\n",
    "    sum_of_losses = mlm_loss + nsp_loss\n",
    "    return mlm_loss, nsp_loss, sum_of_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(bert_model, loss_mlm, loss_nsp, optim, train_iter, iterations):\n",
    "    curr_step = 1\n",
    "    fifth = max(1, iterations // 5)\n",
    "    for (tokens, segments, attention_masks, masked_positions, weights_for_masks,\n",
    "         original_labels_for_masks, nsp_labels) in train_iter:\n",
    "        # forward call and loss calculation\n",
    "        mlm_loss, nsp_loss, sum_of_losses = loss_per_batch(bert_model, loss_mlm, loss_nsp,\n",
    "                                                          vocab_size, tokens, segments,\n",
    "                                                          attention_masks, masked_positions,\n",
    "                                                          weights_for_masks,\n",
    "                                                          original_labels_for_masks, nsp_labels)\n",
    "        # now the backward pass\n",
    "        optim.zero_grad()\n",
    "        sum_of_losses.backward()\n",
    "        optim.step()\n",
    "        if curr_step % fifth == 0:\n",
    "            print(f\"mlm_loss: {mlm_loss:.5f}\\tnsp_loss: {nsp_loss:.5f}\\toverall_loss: {sum_of_losses:.5f}\")\n",
    "        curr_step += 1\n",
    "        if curr_step > iterations:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlm_loss: 6.57617\tnsp_loss: 3.34041\toverall_loss: 9.91658\n"
     ]
    }
   ],
   "source": [
    "# get some GPUs and go crazy :)\n",
    "# might need to to say bert_model.to(device) if you want to make the most out of it with a GPU;\n",
    "train_loop(bert_model, loss_mlm, loss_nsp, adam_optim, train_iter, iterations=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
