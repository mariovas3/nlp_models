{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trains a FeedForward Net on the MNIST dataset;\n",
    "* Uses Cross-Entropy loss;\n",
    "* Uses SGD as optimiser;\n",
    "* I have set a small number of epochs according to the hardware of my local machine;\n",
    "* There is some code commented out showing what to do if you have a GPU;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0390090590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will be using cpu\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvas/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"I will be using {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_data = datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = ToTensor() # PIL format to normalised tensor\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap a DataLoader around the dataset objects to be able to iterate\n",
    "train_dataloader = DataLoader(train_data, batch_size=50, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img is of size torch.Size([1, 28, 28])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "img, label = train_data[0]\n",
    "print(f\"img is of size {img.size()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Choose model class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FFNet,self).__init__()\n",
    "        # flatten object to flatten 28x28 image to a \n",
    "        # 1D contiguous array;\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.my_net = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,10) # digits are from 0 to 9 so 10 elements;\n",
    "        )\n",
    "        \n",
    "    # forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return self.my_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNet(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (my_net): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model = FFNet()#.to(device) # move to GPU if you have one\n",
    "print(model) # has __str__ method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Choose Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Choose opimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go with Stochastic Gradient Descent\n",
    "sgd_optim = torch.optim.SGD(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train loop\n",
    "def train_loop(train_dataloader, model, loss_fn, optim):\n",
    "    size = len(train_dataloader.dataset)\n",
    "    \n",
    "    for batch, (X,y) in enumerate(train_dataloader, start=1):\n",
    "        \n",
    "        # forward pass\n",
    "        preds = model(X)\n",
    "        loss = loss_fn(preds, y)\n",
    "        \n",
    "        # backward pass\n",
    "        # gradients are accumulated so to prevent duble counting\n",
    "        # clear them after each pass;\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        # print progress\n",
    "        if batch%100 == 0:\n",
    "            loss, progress = loss.item(), batch*len(X)\n",
    "            print(f\"loss: {loss:.5f}\\tprogress: {progress}/{size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define test loop to show accuracy and loss after 1 epoch\n",
    "def test_loop(test_dataloader, model, loss_fn):\n",
    "    \n",
    "    test_loss, n_correct = 0,0\n",
    "    size = len(test_dataloader.dataset)\n",
    "    \n",
    "    # here I don't want to track gradients, since this\n",
    "    # is just an evaluation step, to see model performance;\n",
    "    with torch.no_grad():\n",
    "        for (X,y) in test_dataloader:\n",
    "            preds = model(X)\n",
    "            loss = loss_fn(preds, y)\n",
    "            \n",
    "            test_loss += loss * len(X)\n",
    "            n_correct += (preds.argmax(1) == y).sum().item()\n",
    "    print(f\"Test metrics\\nAverage loss: {test_loss/size:.5f}   Accuracy: {n_correct/size:.5f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "----------------------------------\n",
      "loss: 2.30110\tprogress: 5000/60000\n",
      "loss: 2.29647\tprogress: 10000/60000\n",
      "loss: 2.28587\tprogress: 15000/60000\n",
      "loss: 2.28576\tprogress: 20000/60000\n",
      "loss: 2.28133\tprogress: 25000/60000\n",
      "loss: 2.26634\tprogress: 30000/60000\n",
      "loss: 2.26030\tprogress: 35000/60000\n",
      "loss: 2.25930\tprogress: 40000/60000\n",
      "loss: 2.26655\tprogress: 45000/60000\n",
      "loss: 2.23857\tprogress: 50000/60000\n",
      "loss: 2.24953\tprogress: 55000/60000\n",
      "loss: 2.24172\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 2.24181   Accuracy: 0.51380\n",
      "\n",
      "Epoch: 2\n",
      "----------------------------------\n",
      "loss: 2.24509\tprogress: 5000/60000\n",
      "loss: 2.22520\tprogress: 10000/60000\n",
      "loss: 2.21068\tprogress: 15000/60000\n",
      "loss: 2.22011\tprogress: 20000/60000\n",
      "loss: 2.20978\tprogress: 25000/60000\n",
      "loss: 2.21519\tprogress: 30000/60000\n",
      "loss: 2.19611\tprogress: 35000/60000\n",
      "loss: 2.17729\tprogress: 40000/60000\n",
      "loss: 2.18405\tprogress: 45000/60000\n",
      "loss: 2.15704\tprogress: 50000/60000\n",
      "loss: 2.15058\tprogress: 55000/60000\n",
      "loss: 2.12810\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 2.13951   Accuracy: 0.63800\n",
      "\n",
      "Epoch: 3\n",
      "----------------------------------\n",
      "loss: 2.13969\tprogress: 5000/60000\n",
      "loss: 2.10783\tprogress: 10000/60000\n",
      "loss: 2.09408\tprogress: 15000/60000\n",
      "loss: 2.08859\tprogress: 20000/60000\n",
      "loss: 2.06942\tprogress: 25000/60000\n",
      "loss: 2.01268\tprogress: 30000/60000\n",
      "loss: 2.02160\tprogress: 35000/60000\n",
      "loss: 2.03943\tprogress: 40000/60000\n",
      "loss: 2.03530\tprogress: 45000/60000\n",
      "loss: 1.97422\tprogress: 50000/60000\n",
      "loss: 1.98473\tprogress: 55000/60000\n",
      "loss: 1.94846\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 1.93018   Accuracy: 0.67120\n",
      "\n",
      "Epoch: 4\n",
      "----------------------------------\n",
      "loss: 1.93457\tprogress: 5000/60000\n",
      "loss: 1.92418\tprogress: 10000/60000\n",
      "loss: 1.92937\tprogress: 15000/60000\n",
      "loss: 1.77084\tprogress: 20000/60000\n",
      "loss: 1.79875\tprogress: 25000/60000\n",
      "loss: 1.74913\tprogress: 30000/60000\n",
      "loss: 1.75958\tprogress: 35000/60000\n",
      "loss: 1.69589\tprogress: 40000/60000\n",
      "loss: 1.58442\tprogress: 45000/60000\n",
      "loss: 1.58907\tprogress: 50000/60000\n",
      "loss: 1.56348\tprogress: 55000/60000\n",
      "loss: 1.68020\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 1.55382   Accuracy: 0.72030\n",
      "\n",
      "Epoch: 5\n",
      "----------------------------------\n",
      "loss: 1.58277\tprogress: 5000/60000\n",
      "loss: 1.54814\tprogress: 10000/60000\n",
      "loss: 1.36108\tprogress: 15000/60000\n",
      "loss: 1.46473\tprogress: 20000/60000\n",
      "loss: 1.36935\tprogress: 25000/60000\n",
      "loss: 1.43865\tprogress: 30000/60000\n",
      "loss: 1.24829\tprogress: 35000/60000\n",
      "loss: 1.29981\tprogress: 40000/60000\n",
      "loss: 1.29126\tprogress: 45000/60000\n",
      "loss: 1.23157\tprogress: 50000/60000\n",
      "loss: 1.21998\tprogress: 55000/60000\n",
      "loss: 1.22933\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 1.14861   Accuracy: 0.78350\n",
      "\n",
      "Epoch: 6\n",
      "----------------------------------\n",
      "loss: 1.07372\tprogress: 5000/60000\n",
      "loss: 0.99587\tprogress: 10000/60000\n",
      "loss: 1.05893\tprogress: 15000/60000\n",
      "loss: 1.15568\tprogress: 20000/60000\n",
      "loss: 0.98801\tprogress: 25000/60000\n",
      "loss: 0.98798\tprogress: 30000/60000\n",
      "loss: 0.96841\tprogress: 35000/60000\n",
      "loss: 0.89751\tprogress: 40000/60000\n",
      "loss: 1.01015\tprogress: 45000/60000\n",
      "loss: 0.89055\tprogress: 50000/60000\n",
      "loss: 0.86819\tprogress: 55000/60000\n",
      "loss: 0.87439\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 0.88121   Accuracy: 0.82090\n",
      "\n",
      "Epoch: 7\n",
      "----------------------------------\n",
      "loss: 0.80937\tprogress: 5000/60000\n",
      "loss: 0.84585\tprogress: 10000/60000\n",
      "loss: 0.92076\tprogress: 15000/60000\n",
      "loss: 0.74109\tprogress: 20000/60000\n",
      "loss: 0.74037\tprogress: 25000/60000\n",
      "loss: 0.88530\tprogress: 30000/60000\n",
      "loss: 0.72959\tprogress: 35000/60000\n",
      "loss: 0.76113\tprogress: 40000/60000\n",
      "loss: 0.85690\tprogress: 45000/60000\n",
      "loss: 0.89711\tprogress: 50000/60000\n",
      "loss: 0.78926\tprogress: 55000/60000\n",
      "loss: 0.66343\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 0.72292   Accuracy: 0.83860\n",
      "\n",
      "Epoch: 8\n",
      "----------------------------------\n",
      "loss: 0.68897\tprogress: 5000/60000\n",
      "loss: 0.67927\tprogress: 10000/60000\n",
      "loss: 0.60088\tprogress: 15000/60000\n",
      "loss: 0.62946\tprogress: 20000/60000\n",
      "loss: 0.94608\tprogress: 25000/60000\n",
      "loss: 0.59451\tprogress: 30000/60000\n",
      "loss: 0.60949\tprogress: 35000/60000\n",
      "loss: 0.60707\tprogress: 40000/60000\n",
      "loss: 0.75365\tprogress: 45000/60000\n",
      "loss: 0.60882\tprogress: 50000/60000\n",
      "loss: 0.49022\tprogress: 55000/60000\n",
      "loss: 0.57136\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 0.62225   Accuracy: 0.85290\n",
      "\n",
      "Epoch: 9\n",
      "----------------------------------\n",
      "loss: 0.81129\tprogress: 5000/60000\n",
      "loss: 0.60151\tprogress: 10000/60000\n",
      "loss: 0.53293\tprogress: 15000/60000\n",
      "loss: 0.73253\tprogress: 20000/60000\n",
      "loss: 0.63291\tprogress: 25000/60000\n",
      "loss: 0.63116\tprogress: 30000/60000\n",
      "loss: 0.63579\tprogress: 35000/60000\n",
      "loss: 0.64728\tprogress: 40000/60000\n",
      "loss: 0.60462\tprogress: 45000/60000\n",
      "loss: 0.92009\tprogress: 50000/60000\n",
      "loss: 0.55812\tprogress: 55000/60000\n",
      "loss: 0.56533\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 0.55495   Accuracy: 0.86340\n",
      "\n",
      "Epoch: 10\n",
      "----------------------------------\n",
      "loss: 0.52709\tprogress: 5000/60000\n",
      "loss: 0.50569\tprogress: 10000/60000\n",
      "loss: 0.60336\tprogress: 15000/60000\n",
      "loss: 0.59437\tprogress: 20000/60000\n",
      "loss: 0.61263\tprogress: 25000/60000\n",
      "loss: 0.49016\tprogress: 30000/60000\n",
      "loss: 0.45735\tprogress: 35000/60000\n",
      "loss: 0.43550\tprogress: 40000/60000\n",
      "loss: 0.50341\tprogress: 45000/60000\n",
      "loss: 0.75920\tprogress: 50000/60000\n",
      "loss: 0.65379\tprogress: 55000/60000\n",
      "loss: 0.44935\tprogress: 60000/60000\n",
      "Test metrics\n",
      "Average loss: 0.50686   Accuracy: 0.87260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If more compute, go higher with the epochs;\n",
    "epoch = 10\n",
    "\n",
    "for t in range(epoch):\n",
    "    print(f\"Epoch: {t+1}\\n----------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, sgd_optim)\n",
    "    test_loop(test_dataloader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
