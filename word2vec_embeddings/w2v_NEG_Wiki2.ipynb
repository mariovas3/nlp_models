{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo notebook on how to train word2vec embeddings following the negative sampling training procedure proposed by Mikolov et. al.;\n",
    "* The word embeddings are trained on the WikiText2 training dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f290b0fd930>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import get_data as gd\n",
    "import word2vec_NEG as w2v_neg\n",
    "import pickle\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get `torch.utils.data.DataLoader` and `vocab.Vocab` objects;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader, vocab = gd.get_wikitext2_data_neg(\"../data\", vocab_min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of WikiText2 vocab is: 17027\n"
     ]
    }
   ],
   "source": [
    "print(f\"size of WikiText2 vocab is: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the contents of a single batch:\n",
      "\n",
      "torch.Size([512, 1])\n",
      "torch.Size([512, 48])\n",
      "torch.Size([512, 48])\n",
      "torch.Size([512, 48])\n"
     ]
    }
   ],
   "source": [
    "print(\"Check the contents of a single batch:\\n\")\n",
    "# For everything except centers, the shape should be:\n",
    "# (batch_size, (2 * window_size + 2 * window_size * num_negatives));\n",
    "for centers, contexts_and_negatives, coefficients, mask_pads in loader:\n",
    "    print(centers.shape)\n",
    "    print(contexts_and_negatives.shape)\n",
    "    print(coefficients.shape)\n",
    "    print(mask_pads.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsModelNEG(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, **kwargs):\n",
    "        super(EmbeddingsModelNEG, self).__init__(**kwargs)\n",
    "        self.embed_center = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_context = nn.Embedding(vocab_size, embed_size)\n",
    "    \n",
    "    def forward(self, centers, contexts_and_negatives, coefficients):\n",
    "        V = self.embed_center(centers)\n",
    "        U = self.embed_context(contexts_and_negatives)\n",
    "        return torch.bmm(V, U.permute(0, 2, 1)) * coefficients.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embed_size = len(vocab), 100\n",
    "model = EmbeddingsModelNEG(vocab_size, embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the loss;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sigmoid(x):\n",
    "    return torch.log(1 / (1 + torch.exp(- x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLossNEG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingLossNEG, self).__init__()\n",
    "    \n",
    "    def forward(self, prods, mask_pads):\n",
    "        return - (log_sigmoid(prods) * mask_pads.unsqueeze(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = EmbeddingLossNEG()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimise with Adam;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define train loop for 1 epoch;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, loss_fn, optim, loader):\n",
    "    model.train()\n",
    "    fifth = max(1, len(loader) // 5)\n",
    "    batches = len(loader)\n",
    "    tot_loss = 0.\n",
    "    for batch, (centers, contexts_and_negatives, coefficients, \n",
    "                mask_pads) in enumerate(loader, start=1):\n",
    "        prods = model(centers, contexts_and_negatives, coefficients)\n",
    "        loss = loss_fn(prods, mask_pads)\n",
    "        if batch % fifth == 0:\n",
    "            print(f\"train_loss: {loss.item():.5f}\\tprogress: {batch}/{batches}\")\n",
    "        tot_loss += loss.item()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    print(f\"total train loss: {tot_loss:.5f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvas/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py:147: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:115.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 54512.91016\tprogress: 148/741\n",
      "train_loss: 53060.00000\tprogress: 296/741\n",
      "train_loss: 53230.69531\tprogress: 444/741\n",
      "train_loss: 48379.25000\tprogress: 592/741\n",
      "train_loss: 48962.58203\tprogress: 740/741\n",
      "total train loss: 39111040.25000\n",
      "\n",
      "Epoch 2:\n",
      "-------------------------\n",
      "train_loss: 49894.75000\tprogress: 148/741\n",
      "train_loss: 49520.80859\tprogress: 296/741\n",
      "train_loss: 45538.88281\tprogress: 444/741\n",
      "train_loss: 46069.74219\tprogress: 592/741\n",
      "train_loss: 43267.92578\tprogress: 740/741\n",
      "total train loss: 35282268.53711\n",
      "\n",
      "Epoch 3:\n",
      "-------------------------\n",
      "train_loss: 43608.15234\tprogress: 148/741\n",
      "train_loss: 42912.37109\tprogress: 296/741\n",
      "train_loss: 42652.69531\tprogress: 444/741\n",
      "train_loss: 42193.89844\tprogress: 592/741\n",
      "train_loss: 43532.42188\tprogress: 740/741\n",
      "total train loss: 32338957.28711\n",
      "\n",
      "Epoch 4:\n",
      "-------------------------\n",
      "train_loss: 41045.20703\tprogress: 148/741\n",
      "train_loss: 41196.01953\tprogress: 296/741\n",
      "train_loss: 39322.68750\tprogress: 444/741\n",
      "train_loss: 39548.21094\tprogress: 592/741\n",
      "train_loss: 38678.87109\tprogress: 740/741\n",
      "total train loss: 29770734.61719\n",
      "\n",
      "Epoch 5:\n",
      "-------------------------\n",
      "train_loss: 36556.82031\tprogress: 148/741\n",
      "train_loss: 38839.68750\tprogress: 296/741\n",
      "train_loss: 36378.68359\tprogress: 444/741\n",
      "train_loss: 36357.25391\tprogress: 592/741\n",
      "train_loss: 36701.54297\tprogress: 740/741\n",
      "total train loss: 27412736.91211\n",
      "\n",
      "Epoch 6:\n",
      "-------------------------\n",
      "train_loss: 35635.93359\tprogress: 148/741\n",
      "train_loss: 34884.59766\tprogress: 296/741\n",
      "train_loss: 34115.76953\tprogress: 444/741\n",
      "train_loss: 33809.29297\tprogress: 592/741\n",
      "train_loss: 32331.26953\tprogress: 740/741\n",
      "total train loss: 25188021.94238\n",
      "\n",
      "Epoch 7:\n",
      "-------------------------\n",
      "train_loss: 30674.13086\tprogress: 148/741\n",
      "train_loss: 32083.06641\tprogress: 296/741\n",
      "train_loss: 32453.16992\tprogress: 444/741\n",
      "train_loss: 31334.42969\tprogress: 592/741\n",
      "train_loss: 29155.58203\tprogress: 740/741\n",
      "total train loss: 23055579.49219\n",
      "\n",
      "Epoch 8:\n",
      "-------------------------\n",
      "train_loss: 29419.42578\tprogress: 148/741\n",
      "train_loss: 28235.91211\tprogress: 296/741\n",
      "train_loss: 28786.23633\tprogress: 444/741\n",
      "train_loss: 27919.59375\tprogress: 592/741\n",
      "train_loss: 27795.66406\tprogress: 740/741\n",
      "total train loss: 20998673.21045\n",
      "\n",
      "Epoch 9:\n",
      "-------------------------\n",
      "train_loss: 26912.50195\tprogress: 148/741\n",
      "train_loss: 25845.17578\tprogress: 296/741\n",
      "train_loss: 26145.41406\tprogress: 444/741\n",
      "train_loss: 25475.69531\tprogress: 592/741\n",
      "train_loss: 23930.46289\tprogress: 740/741\n",
      "total train loss: 19022796.53467\n",
      "\n",
      "Epoch 10:\n",
      "-------------------------\n",
      "train_loss: 24633.25391\tprogress: 148/741\n",
      "train_loss: 23177.44531\tprogress: 296/741\n",
      "train_loss: 23006.80078\tprogress: 444/741\n",
      "train_loss: 22778.51953\tprogress: 592/741\n",
      "train_loss: 21948.73242\tprogress: 740/741\n",
      "total train loss: 17157360.02832\n",
      "\n",
      "Epoch 11:\n",
      "-------------------------\n",
      "train_loss: 20799.36328\tprogress: 148/741\n",
      "train_loss: 21209.17383\tprogress: 296/741\n",
      "train_loss: 21117.87891\tprogress: 444/741\n",
      "train_loss: 20918.15625\tprogress: 592/741\n",
      "train_loss: 19505.00000\tprogress: 740/741\n",
      "total train loss: 15444095.43604\n",
      "\n",
      "Epoch 12:\n",
      "-------------------------\n",
      "train_loss: 20325.67969\tprogress: 148/741\n",
      "train_loss: 18945.91211\tprogress: 296/741\n",
      "train_loss: 18889.79492\tprogress: 444/741\n",
      "train_loss: 16951.22266\tprogress: 592/741\n",
      "train_loss: 18008.78711\tprogress: 740/741\n",
      "total train loss: 13918497.58594\n",
      "\n",
      "Epoch 13:\n",
      "-------------------------\n",
      "train_loss: 16918.89844\tprogress: 148/741\n",
      "train_loss: 16734.33008\tprogress: 296/741\n",
      "train_loss: 17747.97266\tprogress: 444/741\n",
      "train_loss: 17309.22656\tprogress: 592/741\n",
      "train_loss: 16154.27246\tprogress: 740/741\n",
      "total train loss: 12594926.52588\n",
      "\n",
      "Epoch 14:\n",
      "-------------------------\n",
      "train_loss: 15733.66895\tprogress: 148/741\n",
      "train_loss: 16172.26758\tprogress: 296/741\n",
      "train_loss: 15602.12305\tprogress: 444/741\n",
      "train_loss: 14852.24707\tprogress: 592/741\n",
      "train_loss: 14235.39355\tprogress: 740/741\n",
      "total train loss: 11465377.59839\n",
      "\n",
      "Epoch 15:\n",
      "-------------------------\n",
      "train_loss: 13393.60352\tprogress: 148/741\n",
      "train_loss: 14405.78125\tprogress: 296/741\n",
      "train_loss: 12976.56543\tprogress: 444/741\n",
      "train_loss: 13459.27539\tprogress: 592/741\n",
      "train_loss: 14677.12012\tprogress: 740/741\n",
      "total train loss: 10506891.18921\n",
      "\n",
      "Epoch 16:\n",
      "-------------------------\n",
      "train_loss: 14129.90332\tprogress: 148/741\n",
      "train_loss: 13008.57910\tprogress: 296/741\n",
      "train_loss: 12613.88184\tprogress: 444/741\n",
      "train_loss: 12810.22754\tprogress: 592/741\n",
      "train_loss: 13294.27344\tprogress: 740/741\n",
      "total train loss: 9691728.78760\n",
      "\n",
      "Epoch 17:\n",
      "-------------------------\n",
      "train_loss: 11944.01855\tprogress: 148/741\n",
      "train_loss: 12646.93750\tprogress: 296/741\n",
      "train_loss: 12669.60059\tprogress: 444/741\n",
      "train_loss: 12062.27246\tprogress: 592/741\n",
      "train_loss: 11331.17676\tprogress: 740/741\n",
      "total train loss: 8993212.52808\n",
      "\n",
      "Epoch 18:\n",
      "-------------------------\n",
      "train_loss: 11348.23242\tprogress: 148/741\n",
      "train_loss: 11470.99512\tprogress: 296/741\n",
      "train_loss: 11448.03516\tprogress: 444/741\n",
      "train_loss: 11441.79785\tprogress: 592/741\n",
      "train_loss: 11257.80859\tprogress: 740/741\n",
      "total train loss: 8388856.31885\n",
      "\n",
      "Epoch 19:\n",
      "-------------------------\n",
      "train_loss: 10431.72266\tprogress: 148/741\n",
      "train_loss: 10714.56543\tprogress: 296/741\n",
      "train_loss: 10722.53223\tprogress: 444/741\n",
      "train_loss: 9937.94922\tprogress: 592/741\n",
      "train_loss: 10600.00391\tprogress: 740/741\n",
      "total train loss: 7860697.93091\n",
      "\n",
      "Epoch 20:\n",
      "-------------------------\n",
      "train_loss: 10524.87207\tprogress: 148/741\n",
      "train_loss: 10984.20215\tprogress: 296/741\n",
      "train_loss: 10128.78711\tprogress: 444/741\n",
      "train_loss: 9576.68359\tprogress: 592/741\n",
      "train_loss: 10166.56543\tprogress: 740/741\n",
      "total train loss: 7394542.68506\n",
      "\n",
      "Epoch 21:\n",
      "-------------------------\n",
      "train_loss: 9799.21484\tprogress: 148/741\n",
      "train_loss: 9504.91113\tprogress: 296/741\n",
      "train_loss: 9203.10547\tprogress: 444/741\n",
      "train_loss: 9354.91602\tprogress: 592/741\n",
      "train_loss: 9675.60254\tprogress: 740/741\n",
      "total train loss: 6979817.34570\n",
      "\n",
      "Epoch 22:\n",
      "-------------------------\n",
      "train_loss: 8454.15723\tprogress: 148/741\n",
      "train_loss: 8646.97461\tprogress: 296/741\n",
      "train_loss: 8737.38867\tprogress: 444/741\n",
      "train_loss: 9261.84961\tprogress: 592/741\n",
      "train_loss: 8452.38281\tprogress: 740/741\n",
      "total train loss: 6608343.36279\n",
      "\n",
      "Epoch 23:\n",
      "-------------------------\n",
      "train_loss: 8364.53809\tprogress: 148/741\n",
      "train_loss: 8645.11719\tprogress: 296/741\n",
      "train_loss: 8570.73438\tprogress: 444/741\n",
      "train_loss: 8388.36230\tprogress: 592/741\n",
      "train_loss: 8108.52002\tprogress: 740/741\n",
      "total train loss: 6273753.46167\n",
      "\n",
      "Epoch 24:\n",
      "-------------------------\n",
      "train_loss: 8293.90918\tprogress: 148/741\n",
      "train_loss: 7971.59961\tprogress: 296/741\n",
      "train_loss: 7999.17969\tprogress: 444/741\n",
      "train_loss: 7981.94043\tprogress: 592/741\n",
      "train_loss: 7646.43262\tprogress: 740/741\n",
      "total train loss: 5971063.15601\n",
      "\n",
      "Epoch 25:\n",
      "-------------------------\n",
      "train_loss: 7981.53955\tprogress: 148/741\n",
      "train_loss: 7607.67773\tprogress: 296/741\n",
      "train_loss: 7486.37646\tprogress: 444/741\n",
      "train_loss: 7515.47754\tprogress: 592/741\n",
      "train_loss: 7841.09766\tprogress: 740/741\n",
      "total train loss: 5696286.01013\n",
      "\n",
      "Epoch 26:\n",
      "-------------------------\n",
      "train_loss: 7512.08691\tprogress: 148/741\n",
      "train_loss: 7234.01367\tprogress: 296/741\n",
      "train_loss: 7398.74268\tprogress: 444/741\n",
      "train_loss: 7266.16455\tprogress: 592/741\n",
      "train_loss: 7241.45654\tprogress: 740/741\n",
      "total train loss: 5446110.25452\n",
      "\n",
      "Epoch 27:\n",
      "-------------------------\n",
      "train_loss: 6696.72607\tprogress: 148/741\n",
      "train_loss: 7198.51758\tprogress: 296/741\n",
      "train_loss: 7072.82715\tprogress: 444/741\n",
      "train_loss: 7019.80371\tprogress: 592/741\n",
      "train_loss: 7317.03418\tprogress: 740/741\n",
      "total train loss: 5217899.50415\n",
      "\n",
      "Epoch 28:\n",
      "-------------------------\n",
      "train_loss: 6752.18848\tprogress: 148/741\n",
      "train_loss: 6553.67285\tprogress: 296/741\n",
      "train_loss: 6934.93994\tprogress: 444/741\n",
      "train_loss: 7005.49219\tprogress: 592/741\n",
      "train_loss: 6798.48535\tprogress: 740/741\n",
      "total train loss: 5009314.65857\n",
      "\n",
      "Epoch 29:\n",
      "-------------------------\n",
      "train_loss: 6519.93896\tprogress: 148/741\n",
      "train_loss: 6682.16797\tprogress: 296/741\n",
      "train_loss: 6329.76123\tprogress: 444/741\n",
      "train_loss: 6475.41797\tprogress: 592/741\n",
      "train_loss: 6632.24854\tprogress: 740/741\n",
      "total train loss: 4818280.64893\n",
      "\n",
      "Epoch 30:\n",
      "-------------------------\n",
      "train_loss: 6043.70703\tprogress: 148/741\n",
      "train_loss: 5948.36719\tprogress: 296/741\n",
      "train_loss: 6344.95312\tprogress: 444/741\n",
      "train_loss: 6166.23193\tprogress: 592/741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 6312.71924\tprogress: 740/741\n",
      "total train loss: 4643113.49170\n",
      "\n",
      "Epoch 31:\n",
      "-------------------------\n",
      "train_loss: 6427.05225\tprogress: 148/741\n",
      "train_loss: 6036.92285\tprogress: 296/741\n",
      "train_loss: 6217.67139\tprogress: 444/741\n",
      "train_loss: 5720.13867\tprogress: 592/741\n",
      "train_loss: 6021.55664\tprogress: 740/741\n",
      "total train loss: 4482233.39197\n",
      "\n",
      "Epoch 32:\n",
      "-------------------------\n",
      "train_loss: 5705.34521\tprogress: 148/741\n",
      "train_loss: 5907.08838\tprogress: 296/741\n",
      "train_loss: 5981.23535\tprogress: 444/741\n",
      "train_loss: 6254.97705\tprogress: 592/741\n",
      "train_loss: 5774.42480\tprogress: 740/741\n",
      "total train loss: 4334296.56787\n",
      "\n",
      "Epoch 33:\n",
      "-------------------------\n",
      "train_loss: 5668.79004\tprogress: 148/741\n",
      "train_loss: 6035.21973\tprogress: 296/741\n",
      "train_loss: 6043.55029\tprogress: 444/741\n",
      "train_loss: 5540.29785\tprogress: 592/741\n",
      "train_loss: 5081.78711\tprogress: 740/741\n",
      "total train loss: 4198055.81995\n",
      "\n",
      "Epoch 34:\n",
      "-------------------------\n",
      "train_loss: 5179.75098\tprogress: 148/741\n",
      "train_loss: 5567.91943\tprogress: 296/741\n",
      "train_loss: 5647.65332\tprogress: 444/741\n",
      "train_loss: 5341.61865\tprogress: 592/741\n",
      "train_loss: 5582.86084\tprogress: 740/741\n",
      "total train loss: 4072444.58667\n",
      "\n",
      "Epoch 35:\n",
      "-------------------------\n",
      "train_loss: 5101.93066\tprogress: 148/741\n",
      "train_loss: 5236.60156\tprogress: 296/741\n",
      "train_loss: 5532.22217\tprogress: 444/741\n",
      "train_loss: 5322.52930\tprogress: 592/741\n",
      "train_loss: 5478.75439\tprogress: 740/741\n",
      "total train loss: 3956456.19873\n",
      "\n",
      "Epoch 36:\n",
      "-------------------------\n",
      "train_loss: 5360.11914\tprogress: 148/741\n",
      "train_loss: 5376.73291\tprogress: 296/741\n",
      "train_loss: 5447.65723\tprogress: 444/741\n",
      "train_loss: 5225.79297\tprogress: 592/741\n",
      "train_loss: 5476.12256\tprogress: 740/741\n",
      "total train loss: 3849276.53857\n",
      "\n",
      "Epoch 37:\n",
      "-------------------------\n",
      "train_loss: 4946.21387\tprogress: 148/741\n",
      "train_loss: 4973.76953\tprogress: 296/741\n",
      "train_loss: 5403.23047\tprogress: 444/741\n",
      "train_loss: 5326.18896\tprogress: 592/741\n",
      "train_loss: 5034.56885\tprogress: 740/741\n",
      "total train loss: 3749982.34827\n",
      "\n",
      "Epoch 38:\n",
      "-------------------------\n",
      "train_loss: 4801.70801\tprogress: 148/741\n",
      "train_loss: 4908.34814\tprogress: 296/741\n",
      "train_loss: 5061.98340\tprogress: 444/741\n",
      "train_loss: 5076.56934\tprogress: 592/741\n",
      "train_loss: 5126.69189\tprogress: 740/741\n",
      "total train loss: 3657940.36292\n",
      "\n",
      "Epoch 39:\n",
      "-------------------------\n",
      "train_loss: 4844.68066\tprogress: 148/741\n",
      "train_loss: 4904.92920\tprogress: 296/741\n",
      "train_loss: 4852.23926\tprogress: 444/741\n",
      "train_loss: 4849.36670\tprogress: 592/741\n",
      "train_loss: 4943.65234\tprogress: 740/741\n",
      "total train loss: 3572435.19299\n",
      "\n",
      "Epoch 40:\n",
      "-------------------------\n",
      "train_loss: 4655.76953\tprogress: 148/741\n",
      "train_loss: 4825.47510\tprogress: 296/741\n",
      "train_loss: 4763.57617\tprogress: 444/741\n",
      "train_loss: 4745.94238\tprogress: 592/741\n",
      "train_loss: 4910.77490\tprogress: 740/741\n",
      "total train loss: 3492873.74927\n",
      "\n",
      "Epoch 41:\n",
      "-------------------------\n",
      "train_loss: 4590.67041\tprogress: 148/741\n",
      "train_loss: 4865.31885\tprogress: 296/741\n",
      "train_loss: 4660.94873\tprogress: 444/741\n",
      "train_loss: 4814.95508\tprogress: 592/741\n",
      "train_loss: 4647.75488\tprogress: 740/741\n",
      "total train loss: 3418745.43164\n",
      "\n",
      "Epoch 42:\n",
      "-------------------------\n",
      "train_loss: 4340.12061\tprogress: 148/741\n",
      "train_loss: 4458.54150\tprogress: 296/741\n",
      "train_loss: 4628.88330\tprogress: 444/741\n",
      "train_loss: 4490.60498\tprogress: 592/741\n",
      "train_loss: 4732.31885\tprogress: 740/741\n",
      "total train loss: 3349521.40161\n",
      "\n",
      "Epoch 43:\n",
      "-------------------------\n",
      "train_loss: 4559.22559\tprogress: 148/741\n",
      "train_loss: 4551.93311\tprogress: 296/741\n",
      "train_loss: 4391.07812\tprogress: 444/741\n",
      "train_loss: 4601.65576\tprogress: 592/741\n",
      "train_loss: 4364.01562\tprogress: 740/741\n",
      "total train loss: 3284734.45178\n",
      "\n",
      "Epoch 44:\n",
      "-------------------------\n",
      "train_loss: 4336.36182\tprogress: 148/741\n",
      "train_loss: 4336.76807\tprogress: 296/741\n",
      "train_loss: 4533.20312\tprogress: 444/741\n",
      "train_loss: 4277.04785\tprogress: 592/741\n",
      "train_loss: 4513.25098\tprogress: 740/741\n",
      "total train loss: 3224011.03430\n",
      "\n",
      "Epoch 45:\n",
      "-------------------------\n",
      "train_loss: 4263.22949\tprogress: 148/741\n",
      "train_loss: 4227.23340\tprogress: 296/741\n",
      "train_loss: 4224.17578\tprogress: 444/741\n",
      "train_loss: 4341.19775\tprogress: 592/741\n",
      "train_loss: 4112.27539\tprogress: 740/741\n",
      "total train loss: 3166965.10413\n",
      "\n",
      "Epoch 46:\n",
      "-------------------------\n",
      "train_loss: 4022.72168\tprogress: 148/741\n",
      "train_loss: 3972.57349\tprogress: 296/741\n",
      "train_loss: 4335.38965\tprogress: 444/741\n",
      "train_loss: 4145.54639\tprogress: 592/741\n",
      "train_loss: 4174.78564\tprogress: 740/741\n",
      "total train loss: 3113261.42505\n",
      "\n",
      "Epoch 47:\n",
      "-------------------------\n",
      "train_loss: 3991.18799\tprogress: 148/741\n",
      "train_loss: 3982.82568\tprogress: 296/741\n",
      "train_loss: 4021.47827\tprogress: 444/741\n",
      "train_loss: 4005.23950\tprogress: 592/741\n",
      "train_loss: 4310.15283\tprogress: 740/741\n",
      "total train loss: 3062578.58411\n",
      "\n",
      "Epoch 48:\n",
      "-------------------------\n",
      "train_loss: 4043.12695\tprogress: 148/741\n",
      "train_loss: 4029.60889\tprogress: 296/741\n",
      "train_loss: 4143.65674\tprogress: 444/741\n",
      "train_loss: 3980.60742\tprogress: 592/741\n",
      "train_loss: 4065.73901\tprogress: 740/741\n",
      "total train loss: 3014650.53619\n",
      "\n",
      "Epoch 49:\n",
      "-------------------------\n",
      "train_loss: 4045.37891\tprogress: 148/741\n",
      "train_loss: 4170.92432\tprogress: 296/741\n",
      "train_loss: 4091.04028\tprogress: 444/741\n",
      "train_loss: 3869.96948\tprogress: 592/741\n",
      "train_loss: 4230.32275\tprogress: 740/741\n",
      "total train loss: 2969285.58911\n",
      "\n",
      "Epoch 50:\n",
      "-------------------------\n",
      "train_loss: 3974.36841\tprogress: 148/741\n",
      "train_loss: 3821.55688\tprogress: 296/741\n",
      "train_loss: 3723.38477\tprogress: 444/741\n",
      "train_loss: 4273.18652\tprogress: 592/741\n",
      "train_loss: 3977.86792\tprogress: 740/741\n",
      "total train loss: 2926178.19763\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(50):\n",
    "    print(f\"Epoch {t+1}:\\n-------------------------\")\n",
    "    train_loop(model, loss_fn, optim, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51:\n",
      "-------------------------\n",
      "train_loss: 3965.02368\tprogress: 148/741\n",
      "train_loss: 3837.23486\tprogress: 296/741\n",
      "train_loss: 3869.54492\tprogress: 444/741\n",
      "train_loss: 3679.48779\tprogress: 592/741\n",
      "train_loss: 3939.10498\tprogress: 740/741\n",
      "total train loss: 2885157.80493\n",
      "\n",
      "Epoch 52:\n",
      "-------------------------\n",
      "train_loss: 3884.69849\tprogress: 148/741\n",
      "train_loss: 3973.03247\tprogress: 296/741\n",
      "train_loss: 3890.01270\tprogress: 444/741\n",
      "train_loss: 3732.89160\tprogress: 592/741\n",
      "train_loss: 3902.59619\tprogress: 740/741\n",
      "total train loss: 2846031.04248\n",
      "\n",
      "Epoch 53:\n",
      "-------------------------\n",
      "train_loss: 3710.52319\tprogress: 148/741\n",
      "train_loss: 3775.92578\tprogress: 296/741\n",
      "train_loss: 3891.46875\tprogress: 444/741\n",
      "train_loss: 3876.96191\tprogress: 592/741\n",
      "train_loss: 3725.39087\tprogress: 740/741\n",
      "total train loss: 2808657.16040\n",
      "\n",
      "Epoch 54:\n",
      "-------------------------\n",
      "train_loss: 3904.47681\tprogress: 148/741\n",
      "train_loss: 3773.42065\tprogress: 296/741\n",
      "train_loss: 3655.56763\tprogress: 444/741\n",
      "train_loss: 3837.68921\tprogress: 592/741\n",
      "train_loss: 3894.01099\tprogress: 740/741\n",
      "total train loss: 2772864.74438\n",
      "\n",
      "Epoch 55:\n",
      "-------------------------\n",
      "train_loss: 3683.05322\tprogress: 148/741\n",
      "train_loss: 3748.25146\tprogress: 296/741\n",
      "train_loss: 3817.34546\tprogress: 444/741\n",
      "train_loss: 3767.47290\tprogress: 592/741\n",
      "train_loss: 3691.98877\tprogress: 740/741\n",
      "total train loss: 2738520.69415\n",
      "\n",
      "Epoch 56:\n",
      "-------------------------\n",
      "train_loss: 3741.61230\tprogress: 148/741\n",
      "train_loss: 3634.31006\tprogress: 296/741\n",
      "train_loss: 3826.96777\tprogress: 444/741\n",
      "train_loss: 3725.48682\tprogress: 592/741\n",
      "train_loss: 3725.06177\tprogress: 740/741\n",
      "total train loss: 2705513.38354\n",
      "\n",
      "Epoch 57:\n",
      "-------------------------\n",
      "train_loss: 3566.06372\tprogress: 148/741\n",
      "train_loss: 3636.19678\tprogress: 296/741\n",
      "train_loss: 3484.10840\tprogress: 444/741\n",
      "train_loss: 3751.76172\tprogress: 592/741\n",
      "train_loss: 3694.95874\tprogress: 740/741\n",
      "total train loss: 2673737.52454\n",
      "\n",
      "Epoch 58:\n",
      "-------------------------\n",
      "train_loss: 3363.69897\tprogress: 148/741\n",
      "train_loss: 3421.57349\tprogress: 296/741\n",
      "train_loss: 3639.21777\tprogress: 444/741\n",
      "train_loss: 3587.79810\tprogress: 592/741\n",
      "train_loss: 3347.99756\tprogress: 740/741\n",
      "total train loss: 2643085.96741\n",
      "\n",
      "Epoch 59:\n",
      "-------------------------\n",
      "train_loss: 3336.69092\tprogress: 148/741\n",
      "train_loss: 3505.50073\tprogress: 296/741\n",
      "train_loss: 3626.02612\tprogress: 444/741\n",
      "train_loss: 3442.01074\tprogress: 592/741\n",
      "train_loss: 3574.27148\tprogress: 740/741\n",
      "total train loss: 2613531.78754\n",
      "\n",
      "Epoch 60:\n",
      "-------------------------\n",
      "train_loss: 3433.26318\tprogress: 148/741\n",
      "train_loss: 3477.31934\tprogress: 296/741\n",
      "train_loss: 3482.58398\tprogress: 444/741\n",
      "train_loss: 3539.89429\tprogress: 592/741\n",
      "train_loss: 3558.22607\tprogress: 740/741\n",
      "total train loss: 2584922.10815\n",
      "\n",
      "Epoch 61:\n",
      "-------------------------\n",
      "train_loss: 3406.04907\tprogress: 148/741\n",
      "train_loss: 3415.22290\tprogress: 296/741\n",
      "train_loss: 3568.93140\tprogress: 444/741\n",
      "train_loss: 3427.14355\tprogress: 592/741\n",
      "train_loss: 3505.15503\tprogress: 740/741\n",
      "total train loss: 2557223.44470\n",
      "\n",
      "Epoch 62:\n",
      "-------------------------\n",
      "train_loss: 3132.80127\tprogress: 148/741\n",
      "train_loss: 3193.20093\tprogress: 296/741\n",
      "train_loss: 3601.95679\tprogress: 444/741\n",
      "train_loss: 3444.28613\tprogress: 592/741\n",
      "train_loss: 3611.48535\tprogress: 740/741\n",
      "total train loss: 2530404.48486\n",
      "\n",
      "Epoch 63:\n",
      "-------------------------\n",
      "train_loss: 3471.25073\tprogress: 148/741\n",
      "train_loss: 3463.65015\tprogress: 296/741\n",
      "train_loss: 3418.22998\tprogress: 444/741\n",
      "train_loss: 3598.21631\tprogress: 592/741\n",
      "train_loss: 3311.68774\tprogress: 740/741\n",
      "total train loss: 2504327.91174\n",
      "\n",
      "Epoch 64:\n",
      "-------------------------\n",
      "train_loss: 3440.61133\tprogress: 148/741\n",
      "train_loss: 3322.11182\tprogress: 296/741\n",
      "train_loss: 3316.06152\tprogress: 444/741\n",
      "train_loss: 3344.58057\tprogress: 592/741\n",
      "train_loss: 3392.75439\tprogress: 740/741\n",
      "total train loss: 2479035.12347\n",
      "\n",
      "Epoch 65:\n",
      "-------------------------\n",
      "train_loss: 3246.23511\tprogress: 148/741\n",
      "train_loss: 3451.64087\tprogress: 296/741\n",
      "train_loss: 3279.41357\tprogress: 444/741\n",
      "train_loss: 3185.23071\tprogress: 592/741\n",
      "train_loss: 3407.90430\tprogress: 740/741\n",
      "total train loss: 2454469.35449\n",
      "\n",
      "Epoch 66:\n",
      "-------------------------\n",
      "train_loss: 3201.66016\tprogress: 148/741\n",
      "train_loss: 3188.30420\tprogress: 296/741\n",
      "train_loss: 3255.71484\tprogress: 444/741\n",
      "train_loss: 3497.85547\tprogress: 592/741\n",
      "train_loss: 3236.64404\tprogress: 740/741\n",
      "total train loss: 2430529.87531\n",
      "\n",
      "Epoch 67:\n",
      "-------------------------\n",
      "train_loss: 3271.33691\tprogress: 148/741\n",
      "train_loss: 3310.69653\tprogress: 296/741\n",
      "train_loss: 3128.97144\tprogress: 444/741\n",
      "train_loss: 3088.88354\tprogress: 592/741\n",
      "train_loss: 3029.10571\tprogress: 740/741\n",
      "total train loss: 2407214.91870\n",
      "\n",
      "Epoch 68:\n",
      "-------------------------\n",
      "train_loss: 3298.24683\tprogress: 148/741\n",
      "train_loss: 3241.88354\tprogress: 296/741\n",
      "train_loss: 3248.29565\tprogress: 444/741\n",
      "train_loss: 3138.51611\tprogress: 592/741\n",
      "train_loss: 3167.48267\tprogress: 740/741\n",
      "total train loss: 2384528.86169\n",
      "\n",
      "Epoch 69:\n",
      "-------------------------\n",
      "train_loss: 3104.84863\tprogress: 148/741\n",
      "train_loss: 3203.40332\tprogress: 296/741\n",
      "train_loss: 3140.34668\tprogress: 444/741\n",
      "train_loss: 3117.20386\tprogress: 592/741\n",
      "train_loss: 3175.48340\tprogress: 740/741\n",
      "total train loss: 2362360.42365\n",
      "\n",
      "Epoch 70:\n",
      "-------------------------\n",
      "train_loss: 3003.82080\tprogress: 148/741\n",
      "train_loss: 3105.48926\tprogress: 296/741\n",
      "train_loss: 3411.29077\tprogress: 444/741\n",
      "train_loss: 3227.94751\tprogress: 592/741\n",
      "train_loss: 3068.81641\tprogress: 740/741\n",
      "total train loss: 2340787.67670\n",
      "\n",
      "Epoch 71:\n",
      "-------------------------\n",
      "train_loss: 3226.49512\tprogress: 148/741\n",
      "train_loss: 3011.96021\tprogress: 296/741\n",
      "train_loss: 3110.44238\tprogress: 444/741\n",
      "train_loss: 3110.54028\tprogress: 592/741\n",
      "train_loss: 3137.95679\tprogress: 740/741\n",
      "total train loss: 2319689.98657\n",
      "\n",
      "Epoch 72:\n",
      "-------------------------\n",
      "train_loss: 3010.25757\tprogress: 148/741\n",
      "train_loss: 2855.31299\tprogress: 296/741\n",
      "train_loss: 3110.08643\tprogress: 444/741\n",
      "train_loss: 3147.90161\tprogress: 592/741\n",
      "train_loss: 3177.28833\tprogress: 740/741\n",
      "total train loss: 2299077.20148\n",
      "\n",
      "Epoch 73:\n",
      "-------------------------\n",
      "train_loss: 2872.52075\tprogress: 148/741\n",
      "train_loss: 3105.20679\tprogress: 296/741\n",
      "train_loss: 3092.60376\tprogress: 444/741\n",
      "train_loss: 2926.38354\tprogress: 592/741\n",
      "train_loss: 3149.10352\tprogress: 740/741\n",
      "total train loss: 2278952.58234\n",
      "\n",
      "Epoch 74:\n",
      "-------------------------\n",
      "train_loss: 2935.60327\tprogress: 148/741\n",
      "train_loss: 3132.42285\tprogress: 296/741\n",
      "train_loss: 3037.74634\tprogress: 444/741\n",
      "train_loss: 2962.72095\tprogress: 592/741\n",
      "train_loss: 3115.60620\tprogress: 740/741\n",
      "total train loss: 2259287.02222\n",
      "\n",
      "Epoch 75:\n",
      "-------------------------\n",
      "train_loss: 3064.00000\tprogress: 148/741\n",
      "train_loss: 2908.86523\tprogress: 296/741\n",
      "train_loss: 3103.50635\tprogress: 444/741\n",
      "train_loss: 3109.37524\tprogress: 592/741\n",
      "train_loss: 3181.67505\tprogress: 740/741\n",
      "total train loss: 2240051.51617\n",
      "\n",
      "Epoch 76:\n",
      "-------------------------\n",
      "train_loss: 3014.47925\tprogress: 148/741\n",
      "train_loss: 2913.61621\tprogress: 296/741\n",
      "train_loss: 3192.27783\tprogress: 444/741\n",
      "train_loss: 2916.64990\tprogress: 592/741\n",
      "train_loss: 3055.51270\tprogress: 740/741\n",
      "total train loss: 2221207.26294\n",
      "\n",
      "Epoch 77:\n",
      "-------------------------\n",
      "train_loss: 2886.10425\tprogress: 148/741\n",
      "train_loss: 2990.11548\tprogress: 296/741\n",
      "train_loss: 3171.94165\tprogress: 444/741\n",
      "train_loss: 2998.88794\tprogress: 592/741\n",
      "train_loss: 2919.21606\tprogress: 740/741\n",
      "total train loss: 2202791.67395\n",
      "\n",
      "Epoch 78:\n",
      "-------------------------\n",
      "train_loss: 2932.07739\tprogress: 148/741\n",
      "train_loss: 2885.09106\tprogress: 296/741\n",
      "train_loss: 2923.67480\tprogress: 444/741\n",
      "train_loss: 2997.97974\tprogress: 592/741\n",
      "train_loss: 3023.09180\tprogress: 740/741\n",
      "total train loss: 2184764.11157\n",
      "\n",
      "Epoch 79:\n",
      "-------------------------\n",
      "train_loss: 2852.63501\tprogress: 148/741\n",
      "train_loss: 2999.20996\tprogress: 296/741\n",
      "train_loss: 3008.75732\tprogress: 444/741\n",
      "train_loss: 3111.36353\tprogress: 592/741\n",
      "train_loss: 2983.83276\tprogress: 740/741\n",
      "total train loss: 2167109.44055\n",
      "\n",
      "Epoch 80:\n",
      "-------------------------\n",
      "train_loss: 3002.57324\tprogress: 148/741\n",
      "train_loss: 2884.35474\tprogress: 296/741\n",
      "train_loss: 3061.15234\tprogress: 444/741\n",
      "train_loss: 2901.64209\tprogress: 592/741\n",
      "train_loss: 2852.77832\tprogress: 740/741\n",
      "total train loss: 2149793.83972\n",
      "\n",
      "Epoch 81:\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 2778.05737\tprogress: 148/741\n",
      "train_loss: 2860.36426\tprogress: 296/741\n",
      "train_loss: 2881.45459\tprogress: 444/741\n",
      "train_loss: 2975.36572\tprogress: 592/741\n",
      "train_loss: 3063.57178\tprogress: 740/741\n",
      "total train loss: 2132849.22687\n",
      "\n",
      "Epoch 82:\n",
      "-------------------------\n",
      "train_loss: 2762.65332\tprogress: 148/741\n",
      "train_loss: 2816.11426\tprogress: 296/741\n",
      "train_loss: 2820.40308\tprogress: 444/741\n",
      "train_loss: 2754.27637\tprogress: 592/741\n",
      "train_loss: 2675.73853\tprogress: 740/741\n",
      "total train loss: 2116256.39459\n",
      "\n",
      "Epoch 83:\n",
      "-------------------------\n",
      "train_loss: 2658.85205\tprogress: 148/741\n",
      "train_loss: 2728.62671\tprogress: 296/741\n",
      "train_loss: 2929.54443\tprogress: 444/741\n",
      "train_loss: 3007.12061\tprogress: 592/741\n",
      "train_loss: 2916.16553\tprogress: 740/741\n",
      "total train loss: 2099979.73425\n",
      "\n",
      "Epoch 84:\n",
      "-------------------------\n",
      "train_loss: 2730.79443\tprogress: 148/741\n",
      "train_loss: 2803.50488\tprogress: 296/741\n",
      "train_loss: 2728.15259\tprogress: 444/741\n",
      "train_loss: 2892.21509\tprogress: 592/741\n",
      "train_loss: 2843.65771\tprogress: 740/741\n",
      "total train loss: 2084008.83081\n",
      "\n",
      "Epoch 85:\n",
      "-------------------------\n",
      "train_loss: 2806.46216\tprogress: 148/741\n",
      "train_loss: 2740.72217\tprogress: 296/741\n",
      "train_loss: 2974.87012\tprogress: 444/741\n",
      "train_loss: 2820.44653\tprogress: 592/741\n",
      "train_loss: 2781.03223\tprogress: 740/741\n",
      "total train loss: 2068348.10077\n",
      "\n",
      "Epoch 86:\n",
      "-------------------------\n",
      "train_loss: 2745.83325\tprogress: 148/741\n",
      "train_loss: 2551.92700\tprogress: 296/741\n",
      "train_loss: 2871.71313\tprogress: 444/741\n",
      "train_loss: 2860.04395\tprogress: 592/741\n",
      "train_loss: 2701.72729\tprogress: 740/741\n",
      "total train loss: 2053006.97894\n",
      "\n",
      "Epoch 87:\n",
      "-------------------------\n",
      "train_loss: 2755.26538\tprogress: 148/741\n",
      "train_loss: 2671.66284\tprogress: 296/741\n",
      "train_loss: 2801.93237\tprogress: 444/741\n",
      "train_loss: 2829.89014\tprogress: 592/741\n",
      "train_loss: 2888.07275\tprogress: 740/741\n",
      "total train loss: 2037941.25116\n",
      "\n",
      "Epoch 88:\n",
      "-------------------------\n",
      "train_loss: 2838.35815\tprogress: 148/741\n",
      "train_loss: 2691.81934\tprogress: 296/741\n",
      "train_loss: 2794.52026\tprogress: 444/741\n",
      "train_loss: 2820.05615\tprogress: 592/741\n",
      "train_loss: 2783.74756\tprogress: 740/741\n",
      "total train loss: 2023149.28925\n",
      "\n",
      "Epoch 89:\n",
      "-------------------------\n",
      "train_loss: 2617.77271\tprogress: 148/741\n",
      "train_loss: 2554.43823\tprogress: 296/741\n",
      "train_loss: 2749.27856\tprogress: 444/741\n",
      "train_loss: 2708.68701\tprogress: 592/741\n",
      "train_loss: 2755.24585\tprogress: 740/741\n",
      "total train loss: 2008668.15533\n",
      "\n",
      "Epoch 90:\n",
      "-------------------------\n",
      "train_loss: 2671.46143\tprogress: 148/741\n",
      "train_loss: 2719.98730\tprogress: 296/741\n",
      "train_loss: 2935.04736\tprogress: 444/741\n",
      "train_loss: 2743.60181\tprogress: 592/741\n",
      "train_loss: 2623.50269\tprogress: 740/741\n",
      "total train loss: 1994408.36658\n",
      "\n",
      "Epoch 91:\n",
      "-------------------------\n",
      "train_loss: 2685.84619\tprogress: 148/741\n",
      "train_loss: 2444.99048\tprogress: 296/741\n",
      "train_loss: 2712.09863\tprogress: 444/741\n",
      "train_loss: 2572.09595\tprogress: 592/741\n",
      "train_loss: 2655.25513\tprogress: 740/741\n",
      "total train loss: 1980444.97510\n",
      "\n",
      "Epoch 92:\n",
      "-------------------------\n",
      "train_loss: 2616.03906\tprogress: 148/741\n",
      "train_loss: 2505.36133\tprogress: 296/741\n",
      "train_loss: 2563.60571\tprogress: 444/741\n",
      "train_loss: 2618.02856\tprogress: 592/741\n",
      "train_loss: 2743.21704\tprogress: 740/741\n",
      "total train loss: 1966719.66400\n",
      "\n",
      "Epoch 93:\n",
      "-------------------------\n",
      "train_loss: 2461.83350\tprogress: 148/741\n",
      "train_loss: 2745.98779\tprogress: 296/741\n",
      "train_loss: 2685.46167\tprogress: 444/741\n",
      "train_loss: 2650.06079\tprogress: 592/741\n",
      "train_loss: 2787.38086\tprogress: 740/741\n",
      "total train loss: 1953259.58722\n",
      "\n",
      "Epoch 94:\n",
      "-------------------------\n",
      "train_loss: 2495.35474\tprogress: 148/741\n",
      "train_loss: 2603.08984\tprogress: 296/741\n",
      "train_loss: 2637.37207\tprogress: 444/741\n",
      "train_loss: 2547.10376\tprogress: 592/741\n",
      "train_loss: 2826.51416\tprogress: 740/741\n",
      "total train loss: 1940020.62817\n",
      "\n",
      "Epoch 95:\n",
      "-------------------------\n",
      "train_loss: 2697.90625\tprogress: 148/741\n",
      "train_loss: 2605.06104\tprogress: 296/741\n",
      "train_loss: 2587.84863\tprogress: 444/741\n",
      "train_loss: 2759.01440\tprogress: 592/741\n",
      "train_loss: 2573.46460\tprogress: 740/741\n",
      "total train loss: 1927026.76068\n",
      "\n",
      "Epoch 96:\n",
      "-------------------------\n",
      "train_loss: 2605.88086\tprogress: 148/741\n",
      "train_loss: 2592.78076\tprogress: 296/741\n",
      "train_loss: 2519.20776\tprogress: 444/741\n",
      "train_loss: 2587.15991\tprogress: 592/741\n",
      "train_loss: 2665.17578\tprogress: 740/741\n",
      "total train loss: 1914270.42987\n",
      "\n",
      "Epoch 97:\n",
      "-------------------------\n",
      "train_loss: 2649.77588\tprogress: 148/741\n",
      "train_loss: 2606.41113\tprogress: 296/741\n",
      "train_loss: 2622.99243\tprogress: 444/741\n",
      "train_loss: 2675.54175\tprogress: 592/741\n",
      "train_loss: 2650.60889\tprogress: 740/741\n",
      "total train loss: 1901721.01593\n",
      "\n",
      "Epoch 98:\n",
      "-------------------------\n",
      "train_loss: 2696.41528\tprogress: 148/741\n",
      "train_loss: 2492.45605\tprogress: 296/741\n",
      "train_loss: 2453.66699\tprogress: 444/741\n",
      "train_loss: 2602.25220\tprogress: 592/741\n",
      "train_loss: 2680.86279\tprogress: 740/741\n",
      "total train loss: 1889373.52405\n",
      "\n",
      "Epoch 99:\n",
      "-------------------------\n",
      "train_loss: 2536.96484\tprogress: 148/741\n",
      "train_loss: 2682.58179\tprogress: 296/741\n",
      "train_loss: 2341.00684\tprogress: 444/741\n",
      "train_loss: 2709.64551\tprogress: 592/741\n",
      "train_loss: 2667.11499\tprogress: 740/741\n",
      "total train loss: 1877260.45837\n",
      "\n",
      "Epoch 100:\n",
      "-------------------------\n",
      "train_loss: 2490.42188\tprogress: 148/741\n",
      "train_loss: 2805.22949\tprogress: 296/741\n",
      "train_loss: 2502.76514\tprogress: 444/741\n",
      "train_loss: 2512.86719\tprogress: 592/741\n",
      "train_loss: 2533.62769\tprogress: 740/741\n",
      "total train loss: 1865370.51709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(50, 100):\n",
    "    print(f\"Epoch {t+1}:\\n-------------------------\")\n",
    "    train_loop(model, loss_fn, optim, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize the embeddings (both center and context) as `numpy` objects;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embeds_wiki_centers.npy\", model.embed_center.weight.data.numpy())\n",
    "np.save(\"embeds_wiki_contexts.npy\", model.embed_context.weight.data.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
