{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstrates how to train the word2vec embeddings based on Project Gutenberg books;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f930808c7b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import word2vec_ralated as w2v\n",
    "import get_data as gd\n",
    "import math\n",
    "import pickle\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get `torch.utils.data.DataLoader` and `vocab.Vocab` objects;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A file with that name already exists, if truncate is True I will overwrite it. Continue [y/n]:n\n"
     ]
    }
   ],
   "source": [
    "loader, vocab = gd.get_iter_and_vocab(\"gutenberg_books.txt\", num_books=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1])\n",
      "torch.Size([512, 48])\n",
      "torch.Size([512, 48])\n",
      "torch.Size([512, 48])\n"
     ]
    }
   ],
   "source": [
    "for centers, contexts_and_negatives, coefficients, mask_pads in loader:\n",
    "    print(centers.shape)\n",
    "    print(contexts_and_negatives.shape)\n",
    "    print(coefficients.shape)\n",
    "    print(mask_pads.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156, [1306, 582], [57, 15, 17, 2957, 12031, 2885, 3956, 2976, 1388, 4])\n"
     ]
    }
   ],
   "source": [
    "print(loader.dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, **kwargs):\n",
    "        super(EmbeddingsModel, self).__init__(**kwargs)\n",
    "        self.embed_center = nn.Embedding(vocab_size, embed_size)\n",
    "        self.embed_context = nn.Embedding(vocab_size, embed_size)\n",
    "    \n",
    "    def forward(self, centers, contexts_and_negatives, coefficients):\n",
    "        V = self.embed_center(centers)\n",
    "        U = self.embed_context(contexts_and_negatives)\n",
    "        return torch.bmm(V, U.permute(0, 2, 1)) * coefficients.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size, embed_size = len(vocab), 100\n",
    "model = EmbeddingsModel(vocab_size, embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sigmoid(x):\n",
    "    return torch.log(1 / (1 + torch.exp(- x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmbeddingLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, prods, mask_pads):\n",
    "        return - (log_sigmoid(prods) * mask_pads.unsqueeze(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = EmbeddingLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, loss_fn, optim, loader):\n",
    "    model.train()\n",
    "    fifth = max(1, len(loader) // 5)\n",
    "    batches = len(loader)\n",
    "    tot_loss = 0.\n",
    "    for batch, (centers, contexts_and_negatives, coefficients, \n",
    "                mask_pads) in enumerate(loader, start=1):\n",
    "        prods = model(centers, contexts_and_negatives, coefficients)\n",
    "        loss = loss_fn(prods, mask_pads)\n",
    "        if batch % fifth == 0:\n",
    "            print(f\"train_loss: {loss.item():.5f}\\tprogress: {batch}/{batches}\")\n",
    "        tot_loss += loss.item()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "-------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mvas/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:145: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  Variable._execution_engine.run_backward(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 38919.75391\tprogress: 13/68\n",
      "train_loss: 38238.70312\tprogress: 26/68\n",
      "train_loss: 39641.30078\tprogress: 39/68\n",
      "train_loss: 40672.30469\tprogress: 52/68\n",
      "train_loss: 39984.49609\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 2:\n",
      "-------------------------\n",
      "train_loss: 39401.93750\tprogress: 13/68\n",
      "train_loss: 39378.27344\tprogress: 26/68\n",
      "train_loss: 40067.01172\tprogress: 39/68\n",
      "train_loss: 38317.08594\tprogress: 52/68\n",
      "train_loss: 38314.89453\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 3:\n",
      "-------------------------\n",
      "train_loss: 37429.77344\tprogress: 13/68\n",
      "train_loss: 38819.75000\tprogress: 26/68\n",
      "train_loss: 36880.53516\tprogress: 39/68\n",
      "train_loss: 36576.88672\tprogress: 52/68\n",
      "train_loss: 37050.13672\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 4:\n",
      "-------------------------\n",
      "train_loss: 36849.45703\tprogress: 13/68\n",
      "train_loss: 38280.48438\tprogress: 26/68\n",
      "train_loss: 39131.79297\tprogress: 39/68\n",
      "train_loss: 37650.18359\tprogress: 52/68\n",
      "train_loss: 39036.22656\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 5:\n",
      "-------------------------\n",
      "train_loss: 34131.85938\tprogress: 13/68\n",
      "train_loss: 36266.92188\tprogress: 26/68\n",
      "train_loss: 36889.80469\tprogress: 39/68\n",
      "train_loss: 35051.18750\tprogress: 52/68\n",
      "train_loss: 36521.26562\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 6:\n",
      "-------------------------\n",
      "train_loss: 35090.20312\tprogress: 13/68\n",
      "train_loss: 36357.95703\tprogress: 26/68\n",
      "train_loss: 36220.27734\tprogress: 39/68\n",
      "train_loss: 36634.28516\tprogress: 52/68\n",
      "train_loss: 34846.44141\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 7:\n",
      "-------------------------\n",
      "train_loss: 34003.93359\tprogress: 13/68\n",
      "train_loss: 36202.47656\tprogress: 26/68\n",
      "train_loss: 33597.45312\tprogress: 39/68\n",
      "train_loss: 36676.42969\tprogress: 52/68\n",
      "train_loss: 36265.61328\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 8:\n",
      "-------------------------\n",
      "train_loss: 34431.14453\tprogress: 13/68\n",
      "train_loss: 34949.12109\tprogress: 26/68\n",
      "train_loss: 34023.91797\tprogress: 39/68\n",
      "train_loss: 34162.32031\tprogress: 52/68\n",
      "train_loss: 32121.07812\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 9:\n",
      "-------------------------\n",
      "train_loss: 33975.86719\tprogress: 13/68\n",
      "train_loss: 35940.13281\tprogress: 26/68\n",
      "train_loss: 34901.70703\tprogress: 39/68\n",
      "train_loss: 32251.46484\tprogress: 52/68\n",
      "train_loss: 32439.16406\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 10:\n",
      "-------------------------\n",
      "train_loss: 31856.01758\tprogress: 13/68\n",
      "train_loss: 33366.92578\tprogress: 26/68\n",
      "train_loss: 34598.66797\tprogress: 39/68\n",
      "train_loss: 31485.35352\tprogress: 52/68\n",
      "train_loss: 32865.41016\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 11:\n",
      "-------------------------\n",
      "train_loss: 33161.74609\tprogress: 13/68\n",
      "train_loss: 33291.85938\tprogress: 26/68\n",
      "train_loss: 33199.14844\tprogress: 39/68\n",
      "train_loss: 32485.52148\tprogress: 52/68\n",
      "train_loss: 33644.80469\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 12:\n",
      "-------------------------\n",
      "train_loss: 31575.76953\tprogress: 13/68\n",
      "train_loss: 30132.59375\tprogress: 26/68\n",
      "train_loss: 30650.57422\tprogress: 39/68\n",
      "train_loss: 30837.66016\tprogress: 52/68\n",
      "train_loss: 32263.18359\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 13:\n",
      "-------------------------\n",
      "train_loss: 31484.79297\tprogress: 13/68\n",
      "train_loss: 28848.23828\tprogress: 26/68\n",
      "train_loss: 30649.72266\tprogress: 39/68\n",
      "train_loss: 32211.19141\tprogress: 52/68\n",
      "train_loss: 30634.47461\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 14:\n",
      "-------------------------\n",
      "train_loss: 30340.28906\tprogress: 13/68\n",
      "train_loss: 31443.86133\tprogress: 26/68\n",
      "train_loss: 30639.33789\tprogress: 39/68\n",
      "train_loss: 28676.24609\tprogress: 52/68\n",
      "train_loss: 30892.47656\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 15:\n",
      "-------------------------\n",
      "train_loss: 29207.56836\tprogress: 13/68\n",
      "train_loss: 30892.83203\tprogress: 26/68\n",
      "train_loss: 27972.38672\tprogress: 39/68\n",
      "train_loss: 28667.36914\tprogress: 52/68\n",
      "train_loss: 27049.17578\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 16:\n",
      "-------------------------\n",
      "train_loss: 28159.98047\tprogress: 13/68\n",
      "train_loss: 28753.83203\tprogress: 26/68\n",
      "train_loss: 29022.22852\tprogress: 39/68\n",
      "train_loss: 30301.73828\tprogress: 52/68\n",
      "train_loss: 28902.25586\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 17:\n",
      "-------------------------\n",
      "train_loss: 27873.91992\tprogress: 13/68\n",
      "train_loss: 27627.56250\tprogress: 26/68\n",
      "train_loss: 27670.40234\tprogress: 39/68\n",
      "train_loss: 27737.28516\tprogress: 52/68\n",
      "train_loss: 29273.44531\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 18:\n",
      "-------------------------\n",
      "train_loss: 26849.83594\tprogress: 13/68\n",
      "train_loss: 27272.97266\tprogress: 26/68\n",
      "train_loss: 27834.11133\tprogress: 39/68\n",
      "train_loss: 27486.08984\tprogress: 52/68\n",
      "train_loss: 28657.81055\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 19:\n",
      "-------------------------\n",
      "train_loss: 26518.75586\tprogress: 13/68\n",
      "train_loss: 25974.21094\tprogress: 26/68\n",
      "train_loss: 26990.88867\tprogress: 39/68\n",
      "train_loss: 26804.93945\tprogress: 52/68\n",
      "train_loss: 27008.68555\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 20:\n",
      "-------------------------\n",
      "train_loss: 26114.92383\tprogress: 13/68\n",
      "train_loss: 25984.45898\tprogress: 26/68\n",
      "train_loss: 26070.42578\tprogress: 39/68\n",
      "train_loss: 25742.74805\tprogress: 52/68\n",
      "train_loss: 25386.64844\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 21:\n",
      "-------------------------\n",
      "train_loss: 25805.97266\tprogress: 13/68\n",
      "train_loss: 25910.86133\tprogress: 26/68\n",
      "train_loss: 25361.83789\tprogress: 39/68\n",
      "train_loss: 26314.84570\tprogress: 52/68\n",
      "train_loss: 26005.88086\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 22:\n",
      "-------------------------\n",
      "train_loss: 25254.99805\tprogress: 13/68\n",
      "train_loss: 24110.97656\tprogress: 26/68\n",
      "train_loss: 26269.83594\tprogress: 39/68\n",
      "train_loss: 26073.35352\tprogress: 52/68\n",
      "train_loss: 23931.72461\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 23:\n",
      "-------------------------\n",
      "train_loss: 25661.38086\tprogress: 13/68\n",
      "train_loss: 24036.87891\tprogress: 26/68\n",
      "train_loss: 26215.17188\tprogress: 39/68\n",
      "train_loss: 24352.76172\tprogress: 52/68\n",
      "train_loss: 24331.39062\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 24:\n",
      "-------------------------\n",
      "train_loss: 22942.91406\tprogress: 13/68\n",
      "train_loss: 24835.24023\tprogress: 26/68\n",
      "train_loss: 23757.98633\tprogress: 39/68\n",
      "train_loss: 24522.10547\tprogress: 52/68\n",
      "train_loss: 23973.17578\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 25:\n",
      "-------------------------\n",
      "train_loss: 23297.07422\tprogress: 13/68\n",
      "train_loss: 24125.53906\tprogress: 26/68\n",
      "train_loss: 23374.95312\tprogress: 39/68\n",
      "train_loss: 24108.75586\tprogress: 52/68\n",
      "train_loss: 24085.24805\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 26:\n",
      "-------------------------\n",
      "train_loss: 21712.75781\tprogress: 13/68\n",
      "train_loss: 22535.81445\tprogress: 26/68\n",
      "train_loss: 23398.70508\tprogress: 39/68\n",
      "train_loss: 23422.92188\tprogress: 52/68\n",
      "train_loss: 23400.71094\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 27:\n",
      "-------------------------\n",
      "train_loss: 23010.23633\tprogress: 13/68\n",
      "train_loss: 23173.07227\tprogress: 26/68\n",
      "train_loss: 22754.66406\tprogress: 39/68\n",
      "train_loss: 22812.81836\tprogress: 52/68\n",
      "train_loss: 22135.19531\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 28:\n",
      "-------------------------\n",
      "train_loss: 21565.86719\tprogress: 13/68\n",
      "train_loss: 21189.71680\tprogress: 26/68\n",
      "train_loss: 21537.53711\tprogress: 39/68\n",
      "train_loss: 20649.14062\tprogress: 52/68\n",
      "train_loss: 22244.94141\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 29:\n",
      "-------------------------\n",
      "train_loss: 21189.67969\tprogress: 13/68\n",
      "train_loss: 20517.58594\tprogress: 26/68\n",
      "train_loss: 20750.03125\tprogress: 39/68\n",
      "train_loss: 21933.60156\tprogress: 52/68\n",
      "train_loss: 22590.04492\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 30:\n",
      "-------------------------\n",
      "train_loss: 21188.06250\tprogress: 13/68\n",
      "train_loss: 20540.67969\tprogress: 26/68\n",
      "train_loss: 20828.66211\tprogress: 39/68\n",
      "train_loss: 20925.34766\tprogress: 52/68\n",
      "train_loss: 20180.15625\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 31:\n",
      "-------------------------\n",
      "train_loss: 19513.65234\tprogress: 13/68\n",
      "train_loss: 21379.69727\tprogress: 26/68\n",
      "train_loss: 19181.76758\tprogress: 39/68\n",
      "train_loss: 21465.17969\tprogress: 52/68\n",
      "train_loss: 20112.99609\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 32:\n",
      "-------------------------\n",
      "train_loss: 21047.10156\tprogress: 13/68\n",
      "train_loss: 18735.90039\tprogress: 26/68\n",
      "train_loss: 21060.55664\tprogress: 39/68\n",
      "train_loss: 19148.88281\tprogress: 52/68\n",
      "train_loss: 20121.16211\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 33:\n",
      "-------------------------\n",
      "train_loss: 20297.90820\tprogress: 13/68\n",
      "train_loss: 18812.85742\tprogress: 26/68\n",
      "train_loss: 19265.65039\tprogress: 39/68\n",
      "train_loss: 19675.98633\tprogress: 52/68\n",
      "train_loss: 19838.45117\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 34:\n",
      "-------------------------\n",
      "train_loss: 19009.30859\tprogress: 13/68\n",
      "train_loss: 19099.23438\tprogress: 26/68\n",
      "train_loss: 18292.42969\tprogress: 39/68\n",
      "train_loss: 18977.66797\tprogress: 52/68\n",
      "train_loss: 19690.56641\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 35:\n",
      "-------------------------\n",
      "train_loss: 18815.86523\tprogress: 13/68\n",
      "train_loss: 18316.48047\tprogress: 26/68\n",
      "train_loss: 18923.40625\tprogress: 39/68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 18478.85352\tprogress: 52/68\n",
      "train_loss: 19124.76172\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 36:\n",
      "-------------------------\n",
      "train_loss: 17832.91797\tprogress: 13/68\n",
      "train_loss: 19020.40430\tprogress: 26/68\n",
      "train_loss: 17453.09375\tprogress: 39/68\n",
      "train_loss: 18083.60352\tprogress: 52/68\n",
      "train_loss: 18200.64453\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 37:\n",
      "-------------------------\n",
      "train_loss: 17701.82617\tprogress: 13/68\n",
      "train_loss: 17023.02734\tprogress: 26/68\n",
      "train_loss: 16636.59961\tprogress: 39/68\n",
      "train_loss: 17788.19531\tprogress: 52/68\n",
      "train_loss: 17313.57812\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 38:\n",
      "-------------------------\n",
      "train_loss: 17060.52344\tprogress: 13/68\n",
      "train_loss: 17634.94922\tprogress: 26/68\n",
      "train_loss: 17503.92383\tprogress: 39/68\n",
      "train_loss: 16613.54102\tprogress: 52/68\n",
      "train_loss: 16418.41406\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 39:\n",
      "-------------------------\n",
      "train_loss: 16413.30664\tprogress: 13/68\n",
      "train_loss: 16686.37305\tprogress: 26/68\n",
      "train_loss: 15735.02734\tprogress: 39/68\n",
      "train_loss: 16218.89746\tprogress: 52/68\n",
      "train_loss: 16983.97070\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 40:\n",
      "-------------------------\n",
      "train_loss: 15433.83105\tprogress: 13/68\n",
      "train_loss: 16127.24219\tprogress: 26/68\n",
      "train_loss: 17482.37109\tprogress: 39/68\n",
      "train_loss: 16225.59277\tprogress: 52/68\n",
      "train_loss: 16483.08398\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 41:\n",
      "-------------------------\n",
      "train_loss: 15547.19238\tprogress: 13/68\n",
      "train_loss: 15589.08984\tprogress: 26/68\n",
      "train_loss: 15700.09570\tprogress: 39/68\n",
      "train_loss: 16092.55762\tprogress: 52/68\n",
      "train_loss: 15022.68750\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 42:\n",
      "-------------------------\n",
      "train_loss: 15037.74609\tprogress: 13/68\n",
      "train_loss: 15926.36523\tprogress: 26/68\n",
      "train_loss: 15498.43457\tprogress: 39/68\n",
      "train_loss: 15740.85156\tprogress: 52/68\n",
      "train_loss: 15087.93164\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 43:\n",
      "-------------------------\n",
      "train_loss: 14750.95898\tprogress: 13/68\n",
      "train_loss: 14416.91895\tprogress: 26/68\n",
      "train_loss: 14761.95898\tprogress: 39/68\n",
      "train_loss: 15258.67773\tprogress: 52/68\n",
      "train_loss: 15115.57129\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 44:\n",
      "-------------------------\n",
      "train_loss: 15532.40234\tprogress: 13/68\n",
      "train_loss: 14618.70605\tprogress: 26/68\n",
      "train_loss: 15089.89941\tprogress: 39/68\n",
      "train_loss: 15027.16309\tprogress: 52/68\n",
      "train_loss: 14101.12012\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 45:\n",
      "-------------------------\n",
      "train_loss: 14608.96777\tprogress: 13/68\n",
      "train_loss: 13913.50391\tprogress: 26/68\n",
      "train_loss: 14111.79590\tprogress: 39/68\n",
      "train_loss: 14166.10840\tprogress: 52/68\n",
      "train_loss: 14184.83887\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 46:\n",
      "-------------------------\n",
      "train_loss: 13749.77051\tprogress: 13/68\n",
      "train_loss: 14553.61035\tprogress: 26/68\n",
      "train_loss: 14651.70605\tprogress: 39/68\n",
      "train_loss: 14012.98535\tprogress: 52/68\n",
      "train_loss: 13519.18359\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 47:\n",
      "-------------------------\n",
      "train_loss: 13772.91211\tprogress: 13/68\n",
      "train_loss: 14117.34766\tprogress: 26/68\n",
      "train_loss: 13537.74414\tprogress: 39/68\n",
      "train_loss: 14019.22461\tprogress: 52/68\n",
      "train_loss: 12773.97754\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 48:\n",
      "-------------------------\n",
      "train_loss: 13294.47461\tprogress: 13/68\n",
      "train_loss: 12708.11230\tprogress: 26/68\n",
      "train_loss: 13617.86719\tprogress: 39/68\n",
      "train_loss: 14226.75195\tprogress: 52/68\n",
      "train_loss: 12483.59766\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 49:\n",
      "-------------------------\n",
      "train_loss: 12850.94336\tprogress: 13/68\n",
      "train_loss: 13402.67871\tprogress: 26/68\n",
      "train_loss: 12496.42578\tprogress: 39/68\n",
      "train_loss: 12067.76270\tprogress: 52/68\n",
      "train_loss: 13344.31445\tprogress: 65/68\n",
      "\n",
      "\n",
      "Epoch 50:\n",
      "-------------------------\n",
      "train_loss: 12590.33398\tprogress: 13/68\n",
      "train_loss: 12396.14844\tprogress: 26/68\n",
      "train_loss: 12241.99219\tprogress: 39/68\n",
      "train_loss: 12741.91309\tprogress: 52/68\n",
      "train_loss: 12170.68555\tprogress: 65/68\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in range(50):\n",
    "    print(f\"Epoch {t+1}:\\n-------------------------\")\n",
    "    train_loop(model, loss_fn, optim, loader)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"embeds_v1\", model.embed_center.weight.data.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
